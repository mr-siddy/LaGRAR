{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch_geometric.datasets import WebKB, Planetoid, WikipediaNetwork\n",
    "from torch_geometric.nn import GCNConv, VGAE\n",
    "#from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch_geometric.transforms as T\n",
    "# transform = T.Compose([\n",
    "#     T.RandomNodeSplit(num_val=500, num_test=500),\n",
    "#     T.TargetIndegree(),\n",
    "# ])\n",
    "\n",
    "# dataset = Planetoid(root=\"data\", name=\"CiteSeer\", transform=transform)\n",
    "dataset = WebKB(root=\"/home/siddy/META/data\", name=\"cornell\")\n",
    "data = dataset[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/siddy/META/VAE_Rewire/wandb/run-20231201_162512-pegj9x99</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sidgraph/VAE-Experiments-regularizer/runs/pegj9x99\" target=\"_blank\">vital-firebrand-12</a></strong> to <a href=\"https://wandb.ai/sidgraph/VAE-Experiments-regularizer\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/sidgraph/VAE-Experiments-regularizer\" target=\"_blank\">https://wandb.ai/sidgraph/VAE-Experiments-regularizer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/sidgraph/VAE-Experiments-regularizer/runs/pegj9x99\" target=\"_blank\">https://wandb.ai/sidgraph/VAE-Experiments-regularizer/runs/pegj9x99</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "run_vgae_rewire = wandb.init(\n",
    "        project = \"VAE-Experiments-regularizer\",\n",
    "        config = {\n",
    "            \"architecture\": \"VGAE+Custom_loss+Distributed_backprop\",\n",
    "            \"model\":\"gcn\",\n",
    "            \"dataset\":\"cornell\",\n",
    "            \"epoch\": 500,\n",
    "            \"lr\": 0.01,\n",
    "            \"weight_decay\":1e-3,\n",
    "            \"Batch size\": 1,\n",
    "            \"mask\":1,\n",
    "            \"alpha\":1,\n",
    "            \"beta\":1,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[183, 1703], edge_index=[2, 298], y=[183], train_mask=[183, 10], val_mask=[183, 10], test_mask=[183, 10])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index= data.edge_index\n",
    "num_nodes = data.x.size(0)\n",
    "adj_matrix = torch.zeros((num_nodes, num_nodes), dtype=torch.long)\n",
    "for i in range(edge_index.size(1)):\n",
    "    u, v = int(edge_index[0, i]), int(edge_index[1, i])\n",
    "    adj_matrix[u, v] = 1\n",
    "    adj_matrix[v, u] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_squared = torch.mm(adj_matrix, adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_index(edge_index, adj_matrix, adj_squared):\n",
    "    neg_indices = []\n",
    "    for i in range(edge_index.size(1)):\n",
    "        u, v = int(edge_index[0, i]), int(edge_index[1, i])\n",
    "        #adj_squared = torch.mm(adj_matrix, adj_matrix)\n",
    "        adj_row_u = adj_matrix[u]\n",
    "        adj_row_v = adj_matrix[v]\n",
    "        bitwise_and_result = torch.bitwise_and(adj_squared[u], adj_row_v[v])\n",
    "        sum_bitwise_and_result = torch.sum(bitwise_and_result)\n",
    "        sum_adj_u = torch.sum(adj_row_u)\n",
    "        sum_adj_v = torch.sum(adj_row_v)\n",
    "        neg_index = sum_bitwise_and_result.float() / (sum_adj_u + sum_adj_v)\n",
    "        neg_indices.append(neg_index.item())\n",
    "\n",
    "    return neg_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_index(edge_index, adj_matrix, adj_squared):\n",
    "    pos_indices = []\n",
    "    for i in range(edge_index.size(1)):\n",
    "        u, v = int(edge_index[0, i]), int(edge_index[1, i])\n",
    "        adj_row_u = adj_matrix[u]\n",
    "        adj_row_v = adj_matrix[v]\n",
    "        bitwise_and_result = torch.bitwise_and(adj_row_u, adj_row_v)\n",
    "        sum_bitwise_and_result = torch.sum(bitwise_and_result)\n",
    "        sum_adj_u = torch.sum(adj_row_u)\n",
    "        sum_adj_v = torch.sum(adj_row_v)\n",
    "        pos_index = sum_bitwise_and_result.float() / (sum_adj_u + sum_adj_v)\n",
    "        pos_indices.append(pos_index.item())\n",
    "\n",
    "    return pos_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(aggr='add')\n",
    "        self.lin = Linear(in_channels, out_channels, bias=False)\n",
    "        self.bias = Parameter(torch.empty(out_channels))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin.reset_parameters()\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # what is the shape of inpur x ? - needed [N, in_channels]\n",
    "        # edge indices shape needed is [2, E]\n",
    "\n",
    "        #add self_loops to the adjacency matrix, how to give num nodes?\n",
    "        #edge_index, _ = add_self_loops(edge_index)\n",
    "        #print(edge_index)\n",
    "        # linearly transform node feature matrix\n",
    "        x = self.lin(x)\n",
    "        #x = torch.index_select(input=x, index=edge_index[0], dim=0)\n",
    "        # x_ball = torch.cat([torch.index_select(input=x, index=edge_index[0], dim=0), NOTE THAT IT WILL GIVE INDEX OUT OF RANGE ONE OPTION IS TO GO WITH REINDEXING\n",
    "        #             torch.index_select(input=x, index=edge_index[1], dim=0)],dim=0)\n",
    "        #compute normalization\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt==float('inf')] = 0\n",
    "        #print(deg_inv_sqrt.shape)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        # propagating messages\n",
    "        out = self.propagate(edge_index, x=x, norm=norm)\n",
    "        #out = torch.index_select(input=out, index=min(edge_index[0]), dim=0) #NOTE TRICK IS TO PICK MIN EDGE INDEX AS IT WILL CORRESPOND TO THE CENTER NODE OF THE BALL\n",
    "        # bias\n",
    "        out += self.bias\n",
    "        return torch.squeeze(out)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        # x_j has shape [E, out_channels]\n",
    "        # normalize node features\n",
    "        return norm.view(-1,1) *x_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNNet(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = GCNConv(in_channels, hidden_channels)\n",
    "        self.fc = Linear(hidden_channels, out_channels)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc(self.conv(x, edge_index))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, beta = 1.0, 1.0\n",
    "def custom_objective(sigma_1, sigma_2):\n",
    "  sigma1 = alpha*((sigma_1-0.5)**2) \n",
    "  sigma2 = beta*(sigma_2)\n",
    "  #wandb.log({\"neg_indices\":sigma1})\n",
    "  #wandb.log({\"pos_indices\":sigma2})\n",
    "  return sigma1+sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
    "        self.conv_mu = GCNConv(2 * out_channels, out_channels)\n",
    "        self.conv_logstd = GCNConv(2 * out_channels, out_channels)\n",
    "\n",
    "    def forward(self,x ,edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_channels= dataset.num_classes\n",
    "num_features = dataset.num_features\n",
    "epochs=500\n",
    "num_nodes_train = data.x.size(0)\n",
    "neg_indices = neg_index(data.edge_index, adj_matrix, adj_squared)\n",
    "pos_indices = pos_index(data.edge_index, adj_matrix, adj_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread SystemMonitor:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/siddy/anaconda3/envs/GDL/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/siddy/anaconda3/envs/GDL/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/siddy/.local/lib/python3.8/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 118, in _start\n",
      "    asset.start()\n",
      "  File \"/home/siddy/.local/lib/python3.8/site-packages/wandb/sdk/internal/system/assets/cpu.py\", line 166, in start\n",
      "    self.metrics_monitor.start()\n",
      "  File \"/home/siddy/.local/lib/python3.8/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 168, in start\n",
      "    logger.info(f\"Started {self._process.name}\")\n",
      "AttributeError: 'NoneType' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "model = VGAE(GCNEncoder(num_features, 16))\n",
    "gcn_net = GCNNet(in_channels=data.x.size(1), hidden_channels=64, out_channels=dataset.num_classes)\n",
    "gcn_net = gcn_net.to(device)\n",
    "model = model.to(device)\n",
    "x = data.x.to(device)\n",
    "optimizer1 = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=0.001)\n",
    "optimizer2 = torch.optim.Adam(gcn_net.parameters(), lr=0.01, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, neg_indices, pos_indices):\n",
    "    model.train()\n",
    "    gcn_net.train()\n",
    "    optimizer1.zero_grad()\n",
    "    \n",
    "    # neg_edge_index = negative_sampling(\n",
    "    #     edge_index= data.edge_index,\n",
    "    #     num_nodes= data.x.size(0),\n",
    "    #     num_neg_samples= data.edge_index.size(1)\n",
    "    # )\n",
    "    # neg_indices = neg_index(num_nodes_train, train_data.edge_index)\n",
    "    # pos_indices = pos_index(num_nodes_train, train_data.edge_index)\n",
    "    z = model.encode(x, data.edge_index)\n",
    "    #print(f\"latent space shape: {z.shape}\")\n",
    "    #adj = torch.sigmoid(torch.matmul(z, z.t()))\n",
    "    #print(f\"adj matrix shape: {adj.shape}\")\n",
    "    loss = model.recon_loss(z, train_data.edge_index)\n",
    "    wandb.log({\"recon_loss\":loss.item()})\n",
    "    kl_loss = (1 / data.num_nodes) * model.kl_loss()  # new line\n",
    "    wandb.log({\"kl_loss\":kl_loss.item()})\n",
    "    loss += kl_loss\n",
    "    #adj_binary = (adj > 0.5).float()\n",
    "    #edge_list = adj.nonzero(as_tuple=False)\n",
    "    #edge_list = torch.permute(torch.tensor(edge_list, dtype=torch.long), (1,0)).to(device)\n",
    "    # neg_indices = neg_index(num_nodes_train, train_data.edge_list)\n",
    "    # pos_indices = pos_index(num_nodes_train, train_data.edge_list)\n",
    "    for i in range(len(neg_indices)):\n",
    "        sigma_1 = torch.tensor(neg_indices[i], requires_grad=True)\n",
    "        sigma_2 = torch.tensor(pos_indices[i], requires_grad=True)\n",
    "        objective = custom_objective(sigma_1, sigma_2)\n",
    "        wandb.log({\"obj_loss\":objective.item()})\n",
    "        loss += objective\n",
    "    optimizer2.zero_grad()\n",
    "    out = gcn_net(data.x, data.edge_index)\n",
    "    out_2 = gcn_net(data.x, data.edge_index)\n",
    "    nc_loss = F.cross_entropy(out[data.train_mask[:,1]], data.y[data.train_mask[:,1]])\n",
    "    nc_loss_2 = F.cross_entropy(out_2[data.train_mask[:,1]], data.y[data.train_mask[:,1]])\n",
    "    wandb.log({\"nc_loss\":nc_loss.item()})\n",
    "    loss += nc_loss\n",
    "    loss.backward()\n",
    "    optimizer1.step()\n",
    "    nc_loss_2.backward()\n",
    "    optimizer2.step()\n",
    "    return nc_loss_2, loss\n",
    "\n",
    "\n",
    "def test(test_data):\n",
    "    model.eval()\n",
    "    gcn_net.eval()\n",
    "    with torch.no_grad():\n",
    "        # test_neg_edge_index = negative_sampling(\n",
    "        #     edge_index= test_data.edge_index,\n",
    "        #     num_nodes= test_data.x.size(0),\n",
    "        #     num_neg_samples=test_data.edge_index.size(1)\n",
    "        # )\n",
    "        # #z = model.encode(x, test_data.edge_index)\n",
    "        out = gcn_net(data.x, data.edge_index).argmax(dim=-1)\n",
    "        accs=[]\n",
    "        for mask in [data.train_mask[:,1], data.val_mask[:,1], data.test_mask[:,1]]:\n",
    "            accs.append(int((out[mask] == data.y[mask]).sum())/ int(mask.sum()))\n",
    "        return accs\n",
    "    \n",
    "    #return model.test(z, test_data.edge_index, test_neg_edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nc task computational graph.png'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nc_loss_2, _ = train(data, neg_indices, pos_indices)\n",
    "nc_dot= make_dot(nc_loss_2, params=dict(gcn_net.named_parameters()))\n",
    "nc_dot.render(\"Nc task computational graph\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dot: graph is too large for cairo-renderer bitmaps. Scaling by 0.38154 to fit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'VGAE task computational graph.png'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, loss = train(data, neg_indices, pos_indices)\n",
    "vgae_dot= make_dot(loss, params=dict(model.named_parameters()))\n",
    "vgae_dot.render(\"VGAE task computational graph\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, Loss:670.5535278320312, Train:0.42528735632183906, Val:0.5084745762711864, Test:0.43243243243243246\n",
      "Epoch:2, Loss:888056.25, Train:0.5632183908045977, Val:0.4915254237288136, Test:0.43243243243243246\n",
      "Epoch:3, Loss:402381.5, Train:0.42528735632183906, Val:0.3220338983050847, Test:0.43243243243243246\n",
      "Epoch:4, Loss:2319121.5, Train:0.47126436781609193, Val:0.3220338983050847, Test:0.43243243243243246\n",
      "Epoch:5, Loss:841112.875, Train:0.5862068965517241, Val:0.423728813559322, Test:0.43243243243243246\n",
      "Epoch:6, Loss:2020.6790771484375, Train:0.6781609195402298, Val:0.5084745762711864, Test:0.43243243243243246\n",
      "Epoch:7, Loss:424613.53125, Train:0.6781609195402298, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:8, Loss:2044.8123779296875, Train:0.6781609195402298, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:9, Loss:1673.47021484375, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:10, Loss:2423.095458984375, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:11, Loss:2065.6806640625, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:12, Loss:1429.745849609375, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:13, Loss:1807.578369140625, Train:0.6781609195402298, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:14, Loss:31065.69140625, Train:0.5862068965517241, Val:0.4067796610169492, Test:0.5135135135135135\n",
      "Epoch:15, Loss:59293.16015625, Train:0.5862068965517241, Val:0.4067796610169492, Test:0.5135135135135135\n",
      "Epoch:16, Loss:139084.3125, Train:0.6781609195402298, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:17, Loss:103653.265625, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:18, Loss:73845.2578125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:19, Loss:95344.84375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:20, Loss:117405.4140625, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:21, Loss:110120.1171875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:22, Loss:87928.0234375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:23, Loss:59294.28125, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:24, Loss:30278.3984375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:25, Loss:997.12109375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:26, Loss:1164.8026123046875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:27, Loss:1209.0345458984375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:28, Loss:966.9818115234375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:29, Loss:1074.7567138671875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:30, Loss:1110.8233642578125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:31, Loss:943.0247192382812, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:32, Loss:1005.568603515625, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:33, Loss:1023.1011352539062, Train:0.6781609195402298, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:34, Loss:920.5848999023438, Train:0.6781609195402298, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:35, Loss:957.290283203125, Train:0.6666666666666666, Val:0.423728813559322, Test:0.5135135135135135\n",
      "Epoch:36, Loss:961.1710815429688, Train:0.6666666666666666, Val:0.4406779661016949, Test:0.5135135135135135\n",
      "Epoch:37, Loss:903.8406372070312, Train:0.6666666666666666, Val:0.4406779661016949, Test:0.5135135135135135\n",
      "Epoch:38, Loss:917.4501342773438, Train:0.6666666666666666, Val:0.4406779661016949, Test:0.5135135135135135\n",
      "Epoch:39, Loss:925.4783325195312, Train:0.6666666666666666, Val:0.4406779661016949, Test:0.5135135135135135\n",
      "Epoch:40, Loss:873.035888671875, Train:0.6666666666666666, Val:0.4406779661016949, Test:0.5135135135135135\n",
      "Epoch:41, Loss:900.115966796875, Train:0.6666666666666666, Val:0.4406779661016949, Test:0.5135135135135135\n",
      "Epoch:42, Loss:872.8667602539062, Train:0.6666666666666666, Val:0.423728813559322, Test:0.5135135135135135\n",
      "Epoch:43, Loss:1018.375, Train:0.6666666666666666, Val:0.423728813559322, Test:0.5135135135135135\n",
      "Epoch:44, Loss:902.738525390625, Train:0.6666666666666666, Val:0.423728813559322, Test:0.5135135135135135\n",
      "Epoch:45, Loss:906.807373046875, Train:0.6666666666666666, Val:0.423728813559322, Test:0.5135135135135135\n",
      "Epoch:46, Loss:962.156982421875, Train:0.6666666666666666, Val:0.423728813559322, Test:0.5135135135135135\n",
      "Epoch:47, Loss:882.8842163085938, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:48, Loss:902.0360107421875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:49, Loss:923.08251953125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:50, Loss:861.1769409179688, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:51, Loss:902.1104125976562, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:52, Loss:883.6738891601562, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:53, Loss:853.3055419921875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:54, Loss:2120.044677734375, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:55, Loss:852.7950439453125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:56, Loss:855.429931640625, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:57, Loss:855.4414672851562, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:58, Loss:845.5867919921875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:59, Loss:851.455810546875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:60, Loss:847.9437866210938, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:61, Loss:841.9032592773438, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:62, Loss:842.7074584960938, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:63, Loss:845.2560424804688, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:64, Loss:840.7391967773438, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:65, Loss:839.0068359375, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:66, Loss:841.2477416992188, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:67, Loss:836.7113037109375, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:68, Loss:840.686279296875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:69, Loss:834.785400390625, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:70, Loss:838.5352783203125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:71, Loss:835.2732543945312, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:72, Loss:838.2098388671875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:73, Loss:832.528564453125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:74, Loss:832.8519897460938, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:75, Loss:834.8370971679688, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:76, Loss:834.9830322265625, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:77, Loss:837.711669921875, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:78, Loss:833.6983032226562, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:79, Loss:831.499755859375, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:80, Loss:833.9019165039062, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:81, Loss:832.2407836914062, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:82, Loss:986.3145751953125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:83, Loss:926.2813110351562, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:84, Loss:872.5401611328125, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:85, Loss:937.7820434570312, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:86, Loss:860.293212890625, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:87, Loss:911.714599609375, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:88, Loss:861.4254150390625, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:89, Loss:891.1070556640625, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:90, Loss:843.5069580078125, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:91, Loss:883.9078979492188, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:92, Loss:837.5409545898438, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:93, Loss:867.6414184570312, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:94, Loss:843.6771240234375, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:95, Loss:844.5679321289062, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:96, Loss:841.8314819335938, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:97, Loss:837.1690673828125, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:98, Loss:839.2808227539062, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:99, Loss:843.903564453125, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:100, Loss:833.7592163085938, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:101, Loss:831.8952026367188, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:102, Loss:836.1513061523438, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:103, Loss:823.8472290039062, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:104, Loss:836.8968505859375, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:105, Loss:825.83056640625, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:106, Loss:830.51513671875, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:107, Loss:819.7382202148438, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:108, Loss:832.1777954101562, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:109, Loss:831.3339233398438, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:110, Loss:825.8239135742188, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:111, Loss:822.6199951171875, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:112, Loss:827.6249389648438, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:113, Loss:821.0047607421875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:114, Loss:1419.1553955078125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:115, Loss:988.7954711914062, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:116, Loss:1153.1053466796875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:117, Loss:1016.3778076171875, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:118, Loss:1068.645263671875, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:119, Loss:966.5552978515625, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:120, Loss:1027.17529296875, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:121, Loss:921.3519897460938, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:122, Loss:987.7601928710938, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:123, Loss:904.2962646484375, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:124, Loss:945.5523071289062, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:125, Loss:906.5325317382812, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:126, Loss:897.0706176757812, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:127, Loss:896.4586791992188, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:128, Loss:870.7112426757812, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:129, Loss:882.78076171875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:130, Loss:858.2706909179688, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:131, Loss:863.8161010742188, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:132, Loss:858.417724609375, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:133, Loss:846.380126953125, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:134, Loss:857.1344604492188, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:135, Loss:830.6182250976562, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:136, Loss:852.3622436523438, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:137, Loss:819.552734375, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:138, Loss:855.38134765625, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:139, Loss:811.6051025390625, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:140, Loss:847.243408203125, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:141, Loss:810.4420166015625, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:142, Loss:840.7109375, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:143, Loss:811.2168579101562, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:144, Loss:834.5896606445312, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:145, Loss:820.1990356445312, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:146, Loss:871.5433349609375, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:147, Loss:821.7017211914062, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:148, Loss:842.6511840820312, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:149, Loss:835.3609619140625, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:150, Loss:840.3467407226562, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:151, Loss:828.6806640625, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:152, Loss:834.1598510742188, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:153, Loss:825.6392211914062, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:154, Loss:822.123779296875, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:155, Loss:824.740478515625, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:156, Loss:825.1257934570312, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:157, Loss:816.8687744140625, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:158, Loss:820.3785400390625, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:159, Loss:821.8199462890625, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:160, Loss:810.2308349609375, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:161, Loss:817.17041015625, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:162, Loss:8063.0986328125, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:163, Loss:8053.73828125, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:164, Loss:8060.865234375, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:165, Loss:8056.638671875, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:166, Loss:8056.41650390625, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:167, Loss:22544.884765625, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:168, Loss:831.9461059570312, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:169, Loss:802.6881103515625, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:170, Loss:842.1376953125, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:171, Loss:828.0292358398438, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:172, Loss:816.345947265625, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:173, Loss:831.1766967773438, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:174, Loss:821.1666259765625, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:175, Loss:816.599365234375, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:176, Loss:826.4144897460938, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:177, Loss:817.434326171875, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:178, Loss:821.9453735351562, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:179, Loss:816.2513427734375, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:180, Loss:821.7708740234375, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:181, Loss:815.65234375, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:182, Loss:812.9309692382812, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:183, Loss:818.0287475585938, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:184, Loss:814.6151733398438, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:185, Loss:820.438232421875, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:186, Loss:826.292724609375, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:187, Loss:827.4054565429688, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:188, Loss:813.6973266601562, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:189, Loss:827.0697631835938, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:190, Loss:821.3314819335938, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:191, Loss:815.1520385742188, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:192, Loss:818.3098754882812, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:193, Loss:818.989501953125, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:194, Loss:820.5189208984375, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:195, Loss:815.836181640625, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:196, Loss:813.9976806640625, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:197, Loss:817.0282592773438, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:198, Loss:818.626953125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:199, Loss:814.2932739257812, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:200, Loss:822.8770141601562, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:201, Loss:845.933837890625, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:202, Loss:819.7607421875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:203, Loss:830.8421020507812, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:204, Loss:819.5858154296875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:205, Loss:821.592529296875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:206, Loss:818.409912109375, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:207, Loss:818.5652465820312, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:208, Loss:820.6654663085938, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:209, Loss:818.4022216796875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:210, Loss:814.3564453125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:211, Loss:813.6010131835938, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:212, Loss:816.8776245117188, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:213, Loss:814.1510009765625, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:214, Loss:814.0873413085938, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:215, Loss:814.9647216796875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:216, Loss:819.5670776367188, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:217, Loss:816.5388793945312, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:218, Loss:821.352783203125, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:219, Loss:814.6043701171875, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:220, Loss:807.7789306640625, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:221, Loss:816.12255859375, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:222, Loss:812.1688842773438, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:223, Loss:809.2950439453125, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:224, Loss:809.1634521484375, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:225, Loss:807.2760009765625, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:226, Loss:810.0241088867188, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:227, Loss:805.4328002929688, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:228, Loss:812.7501220703125, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:229, Loss:810.7288208007812, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:230, Loss:801.19970703125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:231, Loss:828.158203125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:232, Loss:810.8870849609375, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:233, Loss:812.29345703125, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:234, Loss:813.2718505859375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:235, Loss:814.0880126953125, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:236, Loss:806.8705444335938, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:237, Loss:811.3077392578125, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:238, Loss:809.8727416992188, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:239, Loss:801.2225952148438, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:240, Loss:116704.078125, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:241, Loss:131189.703125, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:242, Loss:131187.53125, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:243, Loss:131182.03125, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:244, Loss:153141.625, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:245, Loss:145677.3125, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:246, Loss:131186.5625, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:247, Loss:131190.546875, Train:0.6781609195402298, Val:0.5423728813559322, Test:0.5135135135135135\n",
      "Epoch:248, Loss:138436.6875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:249, Loss:174650.671875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:250, Loss:254337.21875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:251, Loss:276061.03125, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:252, Loss:326768.84375, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:253, Loss:341257.21875, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:254, Loss:362984.0625, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:255, Loss:399197.6875, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:256, Loss:406450.15625, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:257, Loss:420939.28125, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:258, Loss:420947.75, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:259, Loss:428183.96875, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:260, Loss:428188.34375, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:261, Loss:428184.71875, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:262, Loss:435425.625, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:263, Loss:435435.71875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:264, Loss:435432.6875, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:265, Loss:442669.34375, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:266, Loss:442679.84375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:267, Loss:442675.125, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:268, Loss:442673.6875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:269, Loss:442664.65625, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:270, Loss:442674.125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:271, Loss:442689.75, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:272, Loss:442667.375, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:273, Loss:442662.09375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:274, Loss:457165.21875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:275, Loss:457160.34375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:276, Loss:457153.6875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:277, Loss:457151.875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:278, Loss:464406.6875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:279, Loss:464417.15625, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:280, Loss:457150.21875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:281, Loss:457157.65625, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:282, Loss:442677.84375, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:283, Loss:428179.5, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:284, Loss:428180.4375, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:285, Loss:428181.34375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:286, Loss:428183.8125, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:287, Loss:428185.15625, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:288, Loss:406443.25, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:289, Loss:377474.375, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:290, Loss:362986.84375, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:291, Loss:355740.03125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:292, Loss:348494.25, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:293, Loss:341259.96875, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:294, Loss:326773.71875, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:295, Loss:319524.5625, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:296, Loss:319531.25, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:297, Loss:290552.5, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:298, Loss:276059.15625, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:299, Loss:268825.625, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:300, Loss:261588.03125, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:301, Loss:261579.0625, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:302, Loss:254338.375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:303, Loss:247092.40625, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:304, Loss:247115.21875, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:305, Loss:247116.09375, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:306, Loss:247084.015625, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:307, Loss:247094.46875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:308, Loss:247111.75, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:309, Loss:247092.796875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:310, Loss:247082.78125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:311, Loss:247101.515625, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:312, Loss:247099.84375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:313, Loss:247086.890625, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:314, Loss:247080.140625, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:315, Loss:247113.875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:316, Loss:247118.625, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:317, Loss:247077.734375, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:318, Loss:254335.453125, Train:0.6896551724137931, Val:0.5423728813559322, Test:0.5135135135135135\n",
      "Epoch:319, Loss:254349.859375, Train:0.6896551724137931, Val:0.5423728813559322, Test:0.5135135135135135\n",
      "Epoch:320, Loss:254322.1875, Train:0.6896551724137931, Val:0.5423728813559322, Test:0.5135135135135135\n",
      "Epoch:321, Loss:254324.375, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:322, Loss:254349.8125, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:323, Loss:254333.578125, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:324, Loss:254324.828125, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:325, Loss:254327.484375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:326, Loss:254332.21875, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:327, Loss:254501.53125, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:328, Loss:15334.6669921875, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:329, Loss:924.7116088867188, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:330, Loss:1278.80712890625, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:331, Loss:4078.726806640625, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:332, Loss:15565.0458984375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:333, Loss:31368.537109375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:334, Loss:25363.80859375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:335, Loss:23978.55078125, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:336, Loss:24555.24609375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:337, Loss:18093.18359375, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:338, Loss:20617.578125, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:339, Loss:14799.7060546875, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:340, Loss:17004.6953125, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:341, Loss:12600.7197265625, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:342, Loss:12498.0595703125, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:343, Loss:12239.1572265625, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:344, Loss:7974.5849609375, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:345, Loss:11648.1455078125, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:346, Loss:5911.70849609375, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:347, Loss:9050.1962890625, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:348, Loss:6250.10498046875, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:349, Loss:13276.783203125, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:350, Loss:28587.576171875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:351, Loss:4739.16748046875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:352, Loss:4937.24560546875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:353, Loss:4311.73828125, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:354, Loss:4080.77392578125, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:355, Loss:3500.591796875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:356, Loss:3633.210205078125, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:357, Loss:3081.53173828125, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:358, Loss:2810.0185546875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:359, Loss:3047.419677734375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:360, Loss:2245.803955078125, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:361, Loss:2549.305419921875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:362, Loss:2311.299560546875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:363, Loss:1962.4310302734375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:364, Loss:2151.14111328125, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:365, Loss:1918.4658203125, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:366, Loss:1763.389404296875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:367, Loss:1746.080322265625, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:368, Loss:1749.060546875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:369, Loss:1460.3316650390625, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:370, Loss:1654.2841796875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:371, Loss:1497.4906005859375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:372, Loss:1303.7130126953125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:373, Loss:1539.825439453125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:374, Loss:1282.1529541015625, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:375, Loss:1303.0955810546875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:376, Loss:1370.1357421875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:377, Loss:1181.6146240234375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:378, Loss:1254.6123046875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:379, Loss:1239.543701171875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:380, Loss:1139.001220703125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:381, Loss:1199.34716796875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:382, Loss:1144.9918212890625, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:383, Loss:1132.3798828125, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:384, Loss:1139.193603515625, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:385, Loss:1107.38720703125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:386, Loss:1105.1236572265625, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:387, Loss:1079.15185546875, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:388, Loss:1088.4725341796875, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:389, Loss:1067.6138916015625, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:390, Loss:1053.5469970703125, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:391, Loss:1063.7022705078125, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:392, Loss:1041.757080078125, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:393, Loss:1038.4166259765625, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:394, Loss:1047.9295654296875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:395, Loss:1021.1156616210938, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:396, Loss:1033.55810546875, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:397, Loss:1028.849365234375, Train:0.6896551724137931, Val:0.5254237288135594, Test:0.5135135135135135\n",
      "Epoch:398, Loss:1011.41845703125, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:399, Loss:1028.1417236328125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:400, Loss:1018.9232788085938, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:401, Loss:1006.9805908203125, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:402, Loss:1017.1160278320312, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:403, Loss:1007.7892456054688, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:404, Loss:1006.8800048828125, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:405, Loss:1017.3571166992188, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:406, Loss:998.1444702148438, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:407, Loss:1006.0968627929688, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:408, Loss:1009.6303100585938, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:409, Loss:997.9620971679688, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:410, Loss:1016.4725952148438, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:411, Loss:995.6953735351562, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:412, Loss:1005.7838745117188, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:413, Loss:1013.9111938476562, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:414, Loss:995.05126953125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:415, Loss:1007.693603515625, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:416, Loss:997.0614624023438, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:417, Loss:1001.1427612304688, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:418, Loss:996.3985595703125, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:419, Loss:1001.670654296875, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:420, Loss:997.3262939453125, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:421, Loss:998.7418212890625, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:422, Loss:994.6689453125, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:423, Loss:995.0010375976562, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:424, Loss:994.5205688476562, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:425, Loss:996.0525512695312, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:426, Loss:998.3104248046875, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:427, Loss:992.2435302734375, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:428, Loss:997.16357421875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:429, Loss:990.705322265625, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:430, Loss:999.4821166992188, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:431, Loss:990.0925903320312, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:432, Loss:994.8416748046875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:433, Loss:990.387451171875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:434, Loss:1000.169677734375, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:435, Loss:993.8250122070312, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:436, Loss:995.0057373046875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:437, Loss:998.8403930664062, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:438, Loss:991.1150512695312, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:439, Loss:1008.883544921875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:440, Loss:993.5071411132812, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:441, Loss:1004.7213134765625, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:442, Loss:998.6260375976562, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:443, Loss:995.141845703125, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:444, Loss:996.9944458007812, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:445, Loss:993.7074584960938, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:446, Loss:995.2479858398438, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:447, Loss:994.0562744140625, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:448, Loss:991.8909912109375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:449, Loss:994.7891235351562, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:450, Loss:990.4833984375, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:451, Loss:997.1917114257812, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:452, Loss:993.4903564453125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:453, Loss:992.8531494140625, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:454, Loss:995.27734375, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:455, Loss:989.1110229492188, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:456, Loss:992.9303588867188, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:457, Loss:988.8502197265625, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:458, Loss:992.1373291015625, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:459, Loss:991.9956665039062, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:460, Loss:988.6070556640625, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:461, Loss:1039.066650390625, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:462, Loss:1003.8873291015625, Train:0.6896551724137931, Val:0.4406779661016949, Test:0.5135135135135135\n",
      "Epoch:463, Loss:1013.1348266601562, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:464, Loss:1020.173828125, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:465, Loss:994.5745849609375, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:466, Loss:1021.9550170898438, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:467, Loss:995.9555053710938, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:468, Loss:1010.17333984375, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:469, Loss:1002.770751953125, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:470, Loss:997.5772094726562, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:471, Loss:1004.8519287109375, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:472, Loss:991.9736938476562, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:473, Loss:1006.0274047851562, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:474, Loss:997.9015502929688, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:475, Loss:996.6766967773438, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:476, Loss:996.9220581054688, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:477, Loss:995.2953491210938, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:478, Loss:997.8855590820312, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:479, Loss:994.220947265625, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:480, Loss:1025.4598388671875, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:481, Loss:995.7830810546875, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:482, Loss:1012.1972045898438, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:483, Loss:1003.9795532226562, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:484, Loss:998.9862060546875, Train:0.6896551724137931, Val:0.4576271186440678, Test:0.5135135135135135\n",
      "Epoch:485, Loss:1004.6795654296875, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:486, Loss:991.8243408203125, Train:0.6896551724137931, Val:0.4745762711864407, Test:0.5135135135135135\n",
      "Epoch:487, Loss:999.9815063476562, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:488, Loss:993.0899658203125, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:489, Loss:997.0406494140625, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:490, Loss:993.9120483398438, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:491, Loss:991.6373901367188, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:492, Loss:994.529296875, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:493, Loss:991.8479614257812, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:494, Loss:990.8963623046875, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:495, Loss:990.4188842773438, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:496, Loss:986.3385620117188, Train:0.6896551724137931, Val:0.5084745762711864, Test:0.5135135135135135\n",
      "Epoch:497, Loss:992.5433349609375, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:498, Loss:984.5880737304688, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:499, Loss:994.0170288085938, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n",
      "Epoch:500, Loss:986.9263305664062, Train:0.6896551724137931, Val:0.4915254237288136, Test:0.5135135135135135\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "epochs=500\n",
    "best_val_acc = final_test_acc = 0\n",
    "times = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    start = time.time()\n",
    "    _, loss= train(data, neg_indices, pos_indices)\n",
    "    wandb.log({\"total_loss\":loss})\n",
    "    train_acc, val_acc, tmp_test_acc = test(data)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    print(f\"Epoch:{epoch}, Loss:{loss}, Train:{train_acc}, Val:{val_acc}, Test:{test_acc}\")\n",
    "    wandb.log({\"train_acc\":train_acc})\n",
    "    wandb.log({\"test_acc\":test_acc})\n",
    "    wandb.log({\"val_acc\":val_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>kl_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>nc_loss</td><td>█▃▃▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>obj_loss</td><td>▁▁▂▁▅▆▅▁▃▁▁▁▃▅▁▁▁▃▅▃▁▁█▃▁▁█▁▁▁▃█▁▁▁▃▁▁▇▄</td></tr><tr><td>recon_loss</td><td>█▇▅▄▄▃▂▃▂▅▃▂▃▂▃▂▂▂▃▂▃▅▃▃▆▃▆▅█▅▅▄▃▂▁▂▁▁▂▁</td></tr><tr><td>test_acc</td><td>▁███████████████████████████████████████</td></tr><tr><td>total_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁██▇████████████████████████████████████</td></tr><tr><td>val_acc</td><td>▁▆▇▅▆▆▆▅▆▆▅▆▆▅▅▅▇▅▆▆▇▆▆▆▆█▇▇▇▆▆▇▆▅▆▇▅▆▅▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>kl_loss</td><td>357.86496</td></tr><tr><td>nc_loss</td><td>0.72592</td></tr><tr><td>obj_loss</td><td>0.25</td></tr><tr><td>recon_loss</td><td>7.64064</td></tr><tr><td>test_acc</td><td>0.51351</td></tr><tr><td>total_loss</td><td>986.92633</td></tr><tr><td>train_acc</td><td>0.68966</td></tr><tr><td>val_acc</td><td>0.49153</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vital-firebrand-12</strong> at: <a href=\"https://wandb.ai/sidgraph/VAE-Experiments-regularizer/runs/pegj9x99\" target=\"_blank\">https://wandb.ai/sidgraph/VAE-Experiments-regularizer/runs/pegj9x99</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231201_162512-pegj9x99/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_vgae_rewire.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
